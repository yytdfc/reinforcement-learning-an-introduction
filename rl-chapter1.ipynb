{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 The Reinforcement Learning Problem\n",
    "\n",
    "## 1.1 Reinforcement Learning\n",
    "\n",
    "**Reinforcement learning**:  \n",
    "a class of solution\n",
    "methods that work well on the class of problems, and the field that studies these problems and their solution methods. \n",
    "\n",
    "**Three most important distinguishing features:**  \n",
    "1. being closed-loop in an essential way  \n",
    "1. not having direct instructions as to what actions to take, and where the consequences of actions, including reward signals play out\n",
    "1. over extended time periods  \n",
    "\n",
    "__*supervised learning*__: learning from a training set of labeled examples provided by a knowledgable external supervisor  \n",
    "__*unsupervised learning*__: finding structure hidden in collections of unlabeled data  \n",
    "\n",
    "**One challenge**: trade-off between exploration and exploitation  \n",
    "**Another key feature**: explicitly considers the whole problem of a goal-directed agent interacting with an uncertain environment  \n",
    "\n",
    "## 1.2 Examples\n",
    "\n",
    "*pass*\n",
    "\n",
    "## 1.3 Elements of Reinforcement Learning\n",
    "\n",
    "__*policy*__: defines the learning agent’s way of behaving at a given time *(primary)*\n",
    "\n",
    "__*reward signal*__: defines the goal in a reinforcement learning problem *(secondary)*\n",
    "\n",
    "__*value function*__: specifies what is good in the long run *(secondary)*\n",
    "\n",
    "__*model*__: inferences to be made about how the environment will\n",
    "behave\n",
    "\n",
    "## 1.4 Limitations and Scope\n",
    "\n",
    "**Compared to evolutionary methods**: \n",
    "Reinforcement learning involve learning while interacting with the environment. Evolutionary methods ignore much of the useful structure of the reinforcement learning problem: they do not use the fact that the policy they are searching for is a function from states to actions; they do not notice which states an individual passes through during its lifetime, or which actions it selects. \n",
    "\n",
    "__*policy gradient methods*__: estimate the directions the parameters should be adjusted in order to most rapidly improve a policy’s performance\n",
    "\n",
    "## 1.5 An Extended Example: Tic-Tac-Toe\n",
    "\n",
    "__*temporal-difference learning*__: update to the estimated value\n",
    "of $s$, denoted $V(s)$, can be written as  \n",
    "<center>$V(s)\\leftarrow V(s)+\\alpha[V(s_0)−V(s)]$</center>  \n",
    "where $\\alpha$ is a small positive fraction called the step-size parameter, which influences\n",
    "the rate of learning.\n",
    "\n",
    "### My anwser:\n",
    "#### Exercise 1.1\n",
    "It would learn more competitive. But would make wrong decision.\n",
    "\n",
    "#### Exercise 1.2\n",
    "The states' number are dramatically decreased, and the learning process is accelerated. Of course we should use symmetriies, and it would be the same if it is fully learning.\n",
    "\n",
    "#### Exercise 1.3\n",
    "It would fall into a local optimal.\n",
    "\n",
    "#### Exercise 1.4\n",
    "*TODO*\n",
    "\n",
    "#### Exercise 1.5\n",
    "Just like AlphaGO\n",
    "\n",
    "## 1.6 Summary\n",
    "*pass*\n",
    "\n",
    "## 1.7 History of Reinforcement Learning\n",
    "*pass*\n",
    "\n",
    "## 1.8 Bibliographical Remarks\n",
    "*pass*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "{'Random Player': 99, 'draw': 107, 'Trained Player': 294}\n",
      "Trained Player win rate 0.7480916030534351\n",
      "Step 1000:\n",
      "{'Random Player': 111, 'draw': 128, 'Trained Player': 261}\n",
      "Trained Player win rate 0.7016129032258065\n",
      "Step 2000:\n",
      "{'Random Player': 108, 'draw': 142, 'Trained Player': 250}\n",
      "Trained Player win rate 0.6983240223463687\n",
      "Step 3000:\n",
      "{'Random Player': 113, 'draw': 112, 'Trained Player': 275}\n",
      "Trained Player win rate 0.7087628865979382\n",
      "Step 4000:\n",
      "{'Random Player': 116, 'draw': 143, 'Trained Player': 241}\n",
      "Trained Player win rate 0.6750700280112045\n",
      "Step 5000:\n",
      "{'Random Player': 83, 'draw': 138, 'Trained Player': 279}\n",
      "Trained Player win rate 0.7707182320441989\n",
      "Step 6000:\n",
      "{'Random Player': 114, 'draw': 126, 'Trained Player': 260}\n",
      "Trained Player win rate 0.6951871657754011\n",
      "Step 7000:\n",
      "{'Random Player': 81, 'draw': 163, 'Trained Player': 256}\n",
      "Trained Player win rate 0.7596439169139466\n",
      "Step 8000:\n",
      "{'Random Player': 88, 'draw': 157, 'Trained Player': 255}\n",
      "Trained Player win rate 0.7434402332361516\n",
      "Step 9000:\n",
      "{'Random Player': 80, 'draw': 157, 'Trained Player': 263}\n",
      "Trained Player win rate 0.7667638483965015\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N_ROW = 4\n",
    "N_SIZE = N_ROW * N_ROW\n",
    "KEY_BASE = np.array([3**i for i in range(N_SIZE)], dtype=np.int16)\n",
    "KEY_SIZE = 3**N_SIZE\n",
    "# side:\n",
    "# 1  for first player\n",
    "# -1 for second player\n",
    "class State(object):\n",
    "    def __init__(self, s = []):\n",
    "        if len(s) == N_SIZE:\n",
    "            self.s_ = np.array(s, dtype=np.int8)\n",
    "        else:\n",
    "            self.s_ = np.zeros(N_SIZE, dtype=np.int8)\n",
    "        self.key_ = np.dot(self.s_+1, KEY_BASE)\n",
    "        self.keys_ = [self.key_]\n",
    "    def __getitem__(self, i):\n",
    "        return self.s_[i]\n",
    "    def repr_point(self, p):\n",
    "        if p==0: return '.'\n",
    "        elif p==1: return 'X'\n",
    "        elif p==-1: return 'O'\n",
    "    def __repr__(self):\n",
    "        return '\\n'+'\\n'.join([' '.join([self.repr_point(self.s_[r*N_ROW+c])\n",
    "                    for c in range(N_ROW)]) for r in range(N_ROW)])+'\\n'\n",
    "    def __call__(self):\n",
    "        return self.s_\n",
    "    def get(self):\n",
    "        return self.s_\n",
    "    def side(self):\n",
    "        ret = -self.s_.sum()\n",
    "        if ret==0: return 1\n",
    "        else: return ret\n",
    "    def judge(self):\n",
    "        row_sum = self.s_.reshape(N_ROW,N_ROW).sum(axis=0)\n",
    "        col_sum = self.s_.reshape(N_ROW,N_ROW).sum(axis=1)\n",
    "        sum_diag_right=self.s_[np.arange(0, N_SIZE, N_ROW+1)].sum()\n",
    "        sum_diag_left =self.s_[np.arange(N_ROW-1, N_SIZE-1, N_ROW-1)].sum()\n",
    "        if row_sum.max() == N_ROW or col_sum.max() == N_ROW\\\n",
    "            or sum_diag_right == N_ROW or sum_diag_left == N_ROW:\n",
    "            return 1\n",
    "        elif row_sum.min() == -N_ROW or col_sum.min() == -N_ROW\\\n",
    "            or sum_diag_right == -N_ROW or sum_diag_left == -N_ROW:\n",
    "            return -1\n",
    "        return 0\n",
    "    def blank(self):\n",
    "        return np.argwhere(self.s_==0).flatten()\n",
    "    def act(self, action, side=None):\n",
    "        if action not in list(range(N_SIZE)) \\\n",
    "            or self.s_[action]!=0:\n",
    "            print('Error action')\n",
    "            return\n",
    "        if not side or side not in {-1, 0, 1}:\n",
    "            side=self.side()\n",
    "        self.s_[action]=side\n",
    "        self.key_+=side*KEY_BASE[action]\n",
    "        self.keys_.append(self.key_)\n",
    "        return self.judge()\n",
    "    def redo(self, action):\n",
    "        if action not in list(range(N_SIZE)) \\\n",
    "            or self.s_[action]==0:\n",
    "            print('Error action redo')\n",
    "            return\n",
    "        self.key_-=self.s_[action]*KEY_BASE[action]\n",
    "        self.s_[action]=0\n",
    "        self.keys_.pop()\n",
    "    def key(self):\n",
    "        return self.key_\n",
    "    def keys(self):\n",
    "        return self.keys_\n",
    "class Player(object):\n",
    "    def __init__(self, name = 'Base Player'):\n",
    "        self.name_ = name\n",
    "    def move(self, state):\n",
    "        return None\n",
    "    def name(self):\n",
    "        return self.name_\n",
    "class RandomPlayer(Player):\n",
    "    def __init__(self, name = 'Random Player'):\n",
    "        self.name_ = name\n",
    "    def move(self, state):\n",
    "        return np.random.choice(state.blank())\n",
    "class TrainedPlayer(Player):\n",
    "    def __init__(self, name = 'Trained Player'):\n",
    "        self.value_ = np.zeros(KEY_SIZE, dtype=np.float32)\n",
    "        self.explore_rate_ = 0.2\n",
    "        self.alpha_ = 0.1\n",
    "        self.name_ = name\n",
    "        self.greedy_ = False\n",
    "    def value(self, state):\n",
    "        return self.value_[state.key()]\n",
    "    def move(self, state, side=None):\n",
    "        if not side: side=state.side()\n",
    "        actions = state.blank()\n",
    "        if not self.greedy_ and np.random.rand()<self.explore_rate_:\n",
    "            return np.random.choice(actions)\n",
    "        values = []\n",
    "        for a in actions:\n",
    "            if state.act(a)==side:\n",
    "                state.redo(a)\n",
    "                return a\n",
    "            values.append(self.value_[state.key()])\n",
    "            state.redo(a)\n",
    "        if side == 1:\n",
    "            return actions[np.argmax(values)]\n",
    "        elif side == -1:\n",
    "            return actions[np.argmin(values)]\n",
    "    def backup(self, state, reward):\n",
    "        self.value_[state.key()]=reward\n",
    "        for k in state.keys()[-2::-1]:\n",
    "            self.value_[k] = self.alpha_ * (reward-self.value_[k])\n",
    "            reward = self.value_[k]\n",
    "    def train(self, epoch=100, other=None): \n",
    "        other = self\n",
    "        self.greedy_=False\n",
    "        for i in range(epoch):\n",
    "            r, s = play_one_round(self, other)\n",
    "            if r!=0:\n",
    "                self.backup(s, r)\n",
    "        self.greedy_=True\n",
    "def play_one_round(player1, player2):\n",
    "    s=State()\n",
    "    while(1):\n",
    "        r=s.act(player1.move(s))\n",
    "        if r!=0:\n",
    "            return r, s\n",
    "        elif(s.blank().size==0):\n",
    "            return 0, s\n",
    "        r=s.act(player2.move(s))\n",
    "        if r!=0:\n",
    "            return r, s\n",
    "        elif(s.blank().size==0):\n",
    "            return 0, s\n",
    "def race(player1, player2, play_round=100):\n",
    "    scores={player1.name():0, player2.name():0, 'draw':0}\n",
    "    for i in range(play_round):\n",
    "        r, _ = play_one_round(player1, player2)\n",
    "        if r==1:\n",
    "            scores[player1.name()]+=1\n",
    "        elif r==-1:\n",
    "            scores[player2.name()]+=1\n",
    "        else:\n",
    "            scores['draw']+=1\n",
    "        player1, player2 = player2, player1\n",
    "    print(scores)\n",
    "    print(player1.name()+' win rate '+str(scores[player1.name()]/(scores[player1.name()]+scores[player2.name()])))\n",
    "    \n",
    "\n",
    "def cmd_play(player):\n",
    "    s=State()\n",
    "    while(1):\n",
    "        if(s.blank().size==0):\n",
    "            return\n",
    "        print(s)\n",
    "        print(s.repr_point(s.side())+' \\'s turn:')\n",
    "        cmd = input()\n",
    "        if cmd=='':\n",
    "            r=s.act(player.move(s))\n",
    "            if r!=0:\n",
    "                print(s)\n",
    "                print('Computer win!')\n",
    "                return\n",
    "        elif cmd=='e':\n",
    "            return\n",
    "        elif len(cmd)==1:\n",
    "            r=s.act(int(cmd))\n",
    "            if r!=0:\n",
    "                print('You win!')\n",
    "        elif len(cmd)==2:\n",
    "            r=s.act((int(cmd[0])-1)*N_ROW+int(cmd[1])-1)\n",
    "            if r!=0:\n",
    "                print('You win!')\n",
    "\n",
    "p = TrainedPlayer()\n",
    "rp = RandomPlayer()\n",
    "# race(RandomPlayer('d'), rp, 500)\n",
    "for i in range(10):\n",
    "    print('Step %d:'%(i*1000))\n",
    "    p.train(1000)\n",
    "    race(p, rp, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
