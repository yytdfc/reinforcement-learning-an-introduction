{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Finite Markov Decision Processes\n",
    "\n",
    "## The Agent-Environment Interface\n",
    "\n",
    "## 3.2 Goals and Rewards\n",
    "$R_t$\n",
    "__*reward hypothesis:*__\n",
    "> That all of what we mean by goals and purposes can be well thought of\n",
    "as the maximization of the expected value of the cumulative sum of a\n",
    "received scalar signal (called reward).\n",
    "\n",
    "## 3.3 Returns\n",
    "return $G_t$ is defined as some specific function of the reward sequence:\n",
    "$$G_t =R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 R_{t+3} + · ·· + \\gamma ^k R_{t+k+1}$$\n",
    "$0\\le\\gamma\\le 1$, called the *discount rate*, determines the present value of future rewards.\n",
    "\n",
    "## 3.4 Unified Notation for Episodic and Continuing Tasks\n",
    "\n",
    "__*episodic tasks*__ v.s. __*continuing tasks*__\n",
    "\n",
    "## 3.5 The Markov Property\n",
    "*pass*\n",
    "\n",
    "## 3.6 Markov Decision Processes\n",
    "__*Markov decision process (MDP)*__  \n",
    "__*finite Markov decision process (finite MDP)*__, state and action spaces are finite\n",
    "\n",
    "\n",
    "\n",
    "## 3.7 Value Function\n",
    "\n",
    "$$v_\\pi(s) = E_\\pi[G_t|S_t =s]$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
